\documentclass[10pt,landscape,letterpaper]{article}
\usepackage{multicol}
%\usepackage{calc}
%\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
%\usepackage{color,graphicx,overpic}
%\usepackage{hyperref}
%\usepackage{tabularx}
\usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage{tabularx}
% For Lin Alg section
\usepackage{tikz}
\usetikzlibrary{calc}
\usepackage{mathtools}

%
% Styling inspired by tex.stackexchange user "Dror" in this posting:
% https://tex.stackexchange.com/questions/8827/preparing-cheat-sheets
%

\thispagestyle{fancy}
\cfoot{\footnotesize Copyright \textcopyright \ 2019 Brian Schubert}
\rfoot{\footnotesize Revised 2019-11-21}
\lhead{MATH 2341\\Differential Equations and Linear Algebra}
\rhead{Fall 2019 \\ Prof. Prasanth George}
\chead{\large \underline{Midterm Exam ``Cheat Sheet"}}
\renewcommand{\headrulewidth}{0pt}
\setlength{\voffset}{-4pt}

\geometry{top=0.7in,left=.5in,right=.5in,bottom=.5in}


% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
%\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
%                                {-1ex plus -.5ex minus -.2ex}%
%                                {1ex plus .2ex}%
%                                {\normalfont\small\bfseries}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

% Notation Definitions
\newcommand{\dd}[1]{\mathrm{d}#1}
\newcommand{\diffop}[2][]{\frac{\dd#1}{\dd #2}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\bvec}[1]{\mathbf{#1}}
\newcommand{\vecspace}[1]{\mathbb{#1}}
\newcommand{\transp}{^\mathrm{T}}

\newcommand\cheatsheetmargin{0.2cm}

% -----------------------------------------------------------------------

\begin{document}
\raggedright
\footnotesize
\begin{multicols}{3}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
     \Large{\underline{Ordinary Differential Equations}} \\
\end{center}

\section{Separable ODEs}
 \begin{description}[style=unboxed,leftmargin=\cheatsheetmargin]
    \item[Form:] \begin{equation*}y' = f(x)g(y)\end{equation*}
    \item[Solved by:] Separating the $x$- and $y$-terms, and then integrating both sides with respect to $x$:  \begin{equation*}
    \boxed{\int \frac{1}{g(y)} \dd y = \int f(x) \, \dd x}
    \end{equation*}
    \item[Newton's Law of Cooling:]
        \begin{equation*}
            \boxed{\diffop[T]{t} \propto T-A \implies T(t) = A+C_1e^{kt}}
        \end{equation*}
\end{description}


\section{First Order Linear ODEs}
\begin{description}[style=unboxed,leftmargin=\cheatsheetmargin]
    \item[Form:]  \begin{equation*}y' + p(x)y = q(x)\end{equation*}
    \item[Solved by:] \begin{align*}
    \boxed{\int \left( I(x)y\right)' \dd x = \int I(x)q(x) \, \dd x \quad
    \text{where } I(x) = e^{\int p(x) \, \dd x}}
    \end{align*}
    \item[Mixing Tank Problems]
    \begin{gather*}
        \mathrm d A / \mathrm d t = r_\text{in}c_\text{in} - r_\text{out}c_\text{out},
        \quad c_\text{out} = A(t)/V(t)\\
        \implies \boxed{\diffop[A]{t} + \frac{A}{V}r_\text{out} = r_\text{in}c_\text{in}}, \quad 
        \diffop[V]{t} = r_\text{in} - r_\text{out} 
    \end{gather*}
\end{description}

\section{Second Order Constant Coefficient Homogeneous ODEs}

\begin{description}[style=unboxed,leftmargin=\cheatsheetmargin]
%    \item[Key Idea:] For a degree $n$ linear ODE, the homogeneous solution will be the linear combinations of $n$ functions, which are found by assuming $y=e^{rt}$ is a solution for some $r$ and substituting this into the ODE to solve for $r$.
    \item[Form:] \begin{equation*}ay'' + by' + cy = 0, \quad a,b,c \in \mathbb{R}\end{equation*}
    \item[Solved by:]
        \begin{equation*}
        y(x) = C_1 e^{r_1 x} + C_2 e^{r_2 x}
        \end{equation*}
        where
         \begin{equation*} ar^2 + br + c = 0 , \qquad
        r_1, r_2 = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}
        \end{equation*}
       
    \item[Case 1.] $b^2 - 4ac > 0$:
        \begin{equation*}
            \boxed{y(x) = C_1 e^{r_1 x} + C_2 e^{r_2 x}}
        \end{equation*}
    \item[Case 2.] $b^2 - 4ac =0$:\\
    Here, $r_1 = r_2 = \frac{-b}{2a}.$ When this occurs, the second solution to the ODE is $\mathbf{x}e^{rt}$:
        \begin{equation*}
            \boxed{y(x) = C_1 e^{r_1 x} + C_2 \mathbf{x}e^{r_1 x}}
        \end{equation*}
    \item[Case 3.] $b^2 - 4ac < 0$:
        In this case, $r_1$ and $r_2$ are complex numbers $p \pm qi$ where $p = \frac{-b}{2a}$ and $q = \frac{\sqrt{4ac - b^2 }}{2a}$. Then,
        \begin{equation*}
        y(x)= c_1e^{(p+qi)x} + c_2 e^{(p-qi) x} = e^{px} \left[c_1 e^{qix} + c_2 e^{-qix}\right]
        \end{equation*}
        which, by Euler's Formula $e^{i\theta} = \cos \theta + i \sin \theta$, becomes
         \begin{equation*}
         \boxed{
             y(x) = e^{px} \left[C_1 \sin \left(qx\right) + C_2 \cos \left(qx\right) \right]
         }
         \end{equation*}
         where $C_1, C_2$ are the real coefficients $c_1 + c_2$ and $ic_1 - ic_2$.
         
         \columnbreak
         
    \item[Spring-Mass-Dashpot System:] \begin{equation*}mx'' + cx' +kx = 0 \end{equation*}
    \begin{description}[style=unboxed,leftmargin=0.1cm]
        \item[Case 1:] $c=0 \implies$ Underdamped Vibration \\
            \qquad \underline{Simple Harmonic Motion}
            \begin{align*}
                x(t) &= C_1 \cos \omega t + C_2 \sin \omega t, \quad \omega = \sqrt{\frac{k}{m}}\\
                \implies x(t) &= \boxed{A\cos (\omega t - \theta)}
            \end{align*}
            where
            \begin{gather*}
                \boxed{A = \sqrt{C_1^2 + C_2^2}} \\
                \boxed{\theta = \begin{cases} 
                    \tan^{-1} \frac{C_2}{C_1} & \text {if $\theta$ is in Q1} \\
                    \tan^{-1} \frac{C_2}{C_1} + \phantom{2}\pi& \text {if $\theta$ is in Q2 or Q3} \\
                    \tan^{-1} \frac{C_2}{C_1} + 2\pi & \text {if $\theta$ is in Q4}
                \end{cases}}
            \end{gather*}
        \item[Case 2a:] $c^2 > 4mk \implies$ ``Overdamped" \\ 
            \qquad \underline{Oscillations die rapidly}
            \begin{equation*}
                 \boxed{x(t) = C_1 e^{r_1 t} + C_2 e^{r_2 t}}
            \end{equation*}
        \item[Case 2b:] $c^2 = 4mk \implies$ ``Critically Damped Oscillation"
            \begin{equation*}
            \boxed{x(t) = C_1 e^{r_1 t} + C_2 \mathbf{t}e^{r_2 t}}
            \end{equation*}
         \item[Case 2c:] $c^2 < 4mk$
        \begin{gather*}
        r_1, r_2 = \underbrace{\frac{-c}{2m}}_{p} \pm i\underbrace{\frac{\sqrt{4mk - c^2}}{2m}}_{\mu: \text{ pseduo frequency}} \\
        \boxed{x(t) = e^{px} \left[C_1 \sin \left(\mu t\right) + C_2 \cos \left(\mu t\right) \right]}
        \end{gather*}
    \end{description}
\end{description}


\section{Second Order Constant Coefficient \underline{Non-}Homogeneous ODEs}
\begin{description}[style=unboxed,leftmargin=\cheatsheetmargin]
    \item[Form:]  \begin{equation*} ay'' + by' + cy = \mathbf{f(x)}, \quad a,b,c \in \mathbb{R}\end{equation*}
    where $f$ is a continuous function of $x$.
    \item[Solved by:]
    \begin{equation*} \boxed{y(x) = y_n(x) + y_p(x)}\end{equation*}
    where 
    \begin{itemize}
        \item$y_n$ is the homogeneous (null) solution to $ay'' + by' + cy = 0.$
        \item$y_p$ is the particular solution to for the non-homogeneous $f$.
    \end{itemize}
    \item[To Solve for $y_p$:]
    \begin{enumerate}
        \item ``Guess" $y_p$ based on the function $f$.
        \item Plug in the guess for $y_p$ into $ay'' + by' + cy = f(x)$
        \item Solve for $y_p$.
    \end{enumerate}
\end{description}
    
    % Split description into two descriptions to allow the second half to be
    % included in the third column's "minipage"
    \columnbreak
    
% Group the third column in a "minipage" to ensure all element remain on the same page
\begin{minipage}{\columnwidth}
\begin{description}[style=unboxed,leftmargin=\cheatsheetmargin]
    \renewcommand{\arraystretch}{1.2}
    \item \begin{tabular}{|c|c|}
        \hline
        \textbf{Non-homogeneous $f(t)$} & \textbf{``Guess" for $y_p$} \\
        \hline
        $ae^{kt}$ & $Ae^{kt}$ \\
        $a\cos k t$ or $a\sin k t$ &  $A\cos k + B\sin k t$ \\
        Polynomial of degree $n$ & Polynomial of degree $n$ \\
        $x^ne^{kt}$ & $(x^n + x^{n-1} +\cdots+ x^1)e^{kt}$ \\
        \hline
    \end{tabular}
    \item[Important:]
        If any term of $y_p$ is linearly dependent with a term of $y_n$, multiply the term of $y_p$ by $t$ until it is linearly independent with all other terms.
   \item[Forced Mechanical Vibrations]
   \begin{equation*}
   mx'' + cx' + kx = f(t), \quad f(t) \text{ is an external force}
   \end{equation*}
   \begin{itemize}
       \item$x_n$ is the ``\underline{transient solution}"; approaches 0 and $t \to \infty$.
       \item$x_p$ is the ``\underline{steady periodic solution}" or ``steady state solution"; keeps oscillating as $t\to \infty$.
   \end{itemize}
    \item[Resonance] Occurs when the frequency of the force $\omega_0$ equals the natural frequency $\omega = \sqrt{k/m}$.
    \begin{equation*}
        f(t) = a\cos\left( \omega_0 t\right) \implies x_p = A\mathbf{t}\sin \omega_0 t + B\mathbf{t}\cos \omega_0 t
    \end{equation*}
\end{description}


\section{The Laplace Transform}
\textit{\scriptsize * Table of common transformations separate from this document.}

Let $f$ be a continuous function of $t$, where $t \in [0, \infty)$. We define the Laplace Transform $\mathcal L\{f(t)\}$ by
\begin{equation*}
\mathcal L\{f(t)\} = \int_0^\infty e^{-st} f(t) \, \mathrm dt
\end{equation*}
where $s$ is the complex ``frequency" parameter.

\subsection{The Heaviside Step Function}
``Turn a function on at time $t$"
\begin{gather*}
u(t-a) := \begin{cases}
    0 & t < a \\
    1 & t \geq a \\
\end{cases}, \qquad
\diffop{t} u(t-a) =: \ \delta(t-a)\\
f(t)  = \begin{cases}
f_1(t) & t \in [0, t_1) \\
f_2(t) & t \in [t_1, t_2) \\
 & \vdots\\
\end{cases} = \begin{array}{rl} 
    f_1(t) 	&+ (f_2(t) - f_1(t))u(t-t_1) \\ 
            &+ (f_3(t) - f_2(t))u(t-t_2) \\
            &+ \, \cdots
\end{array}
\end{gather*}

\subsection{The Dirac Delta Function}

\begin{gather*}
\delta(t-a) := \begin{cases}
0 & t \neq a \\
1 & t = a \\
\end{cases} \\
\int_{-\infty}^\infty \delta(t-a) \, \mathrm dt = 1, \quad \int_{-\infty}^\infty \delta(t-a) f(t) \, \mathrm dt = f(a)
\end{gather*}

\subsection{Shifting Theorems}
If $\mathcal{L}\{f(t)\} = F(s)$, then
\begin{enumerate}
    \item $\mathcal{L}\{e^{at}f(t)\} = F(s-a)$
    \item $\mathcal{L}\{u(t-a)(f(t-a))\} = e^{-as}F(s)$
\end{enumerate}

\subsection{Partial Fractions}

\begin{equation*}
\frac{1}{(s)(s-1)^2(s^2 -2s + 3)} = \frac{A}{s} + \frac{B}{s-1} + \frac{C}{(s-1)^2} + \frac{Ds + E}{(s^2 -2s + 3)}
\end{equation*}

%\begin{center}
%    \Large{\underline{Linear Algebra}} \\
%\end{center}

\end{minipage} % End Column 3

\end{multicols}

\pagebreak
\thispagestyle{fancy}

\begin{multicols}{3}
    
% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
    \Large{\underline{Linear Algebra}} \\
\end{center}

\section{Matrix Algebra}
For any two matrices $\matr{A} \in \vecspace{R}^{m_a \times n_a}$ and $\matr{B} \in \vecspace{R}^{m_b \times n_b}$
\begin{description}
    \item[Matrix Addition] The matrix $\matr{A+B}$ exists if and only if $m_a = m_b$ and $n_a = n_b$. Addition/subtraction then occurs elementwise.
    \item[Scalar Multiplication] The matrix $c\matr{A}$ exists for any scalar $c$. The scalar $c$ multiplies each element of $\matr{A}$.
\end{description}
\vspace{-8pt}
\subsubsection{Matrix Multiplication}
Each column of $\matr{AB}$ is a combination of the columns of $\matr{A}$:

\begin{equation*}
\matr{A} \, \begin{bmatrix}
\vert & & \vert \\
\bvec{b_1} & \cdots & \bvec{b_p} \\
\vert & & \vert
\end{bmatrix} = \begin{bmatrix}
\vert & & \vert \\
\matr{A}\bvec{b_1} & \cdots & \matr{A}\bvec{b_p} \\
\vert & & \vert
\end{bmatrix}
\end{equation*}

\section{The Determinant}
Geometrically, the determinant is the factor by which the linear transformation associated with a matrix ``scales'' $n$-dimensional space
%The determinant of a matrix is usually denoted $\det \matr{A}$ or $|\matr{A}|$. Geometrically, it is the factor by which the linear transformation associated with a matrix ``scales'' $n$-dimensional space. It may be visualized as the volume of the $n$-dimensional parallelopiped formed by the columns of the matrix.
\vspace{-10pt}
\begin{center}
    \begin{tikzpicture}[scale=0.4]
    % untransformed grid behind transformed grid
    \begin{scope}
    % background grid
    \draw[scale=1,gray!40] (-1.9,-0.9) grid (8.9,3.9);
    \draw[->,thick, gray, thick] (0,-1)--(0,4) node[above]{$y$};        
    \end{scope}
    
    % transformed grid (slanted 45 degrees, x scaled by 2)
    \begin{scope}[xslant=tan 45, yscale=2, xscale=3]
    \coordinate (i) at (1,0);
    \coordinate (j) at (0,1);
    
    % transformed grid
    \draw[scale=1,cyan] (-1.9,-0.5) grid (2.3, 1.9);
    \draw[->, very thick, black!10!cyan] (-1,0) -- (3,0) node[right]{$x$};
    \draw[->, very thick, black!10!cyan] (0,-0.5) -- (0,2) node[above]{$y$};  
    
    \draw[-latex, thick, draw=black!50!cyan] (0,0) -- (i) node [midway, below] {$\bvec{i}' = \begin{bsmallmatrix}3\\0\end{bsmallmatrix}$};
    \draw[-latex, thick,  draw=black!50!cyan] (0,0) -- (j) node [midway, above, rotate=45] {$\bvec{j}' = \begin{bsmallmatrix}2\\2\end{bsmallmatrix}$};
    
    \filldraw[draw=none, fill=cyan, fill opacity=0.2] (0,0) rectangle (1,1) node[opacity=1, pos=0.5] {$\det \matr{A}$};
    \end{scope}
    
    \filldraw[white] (-2,-1) rectangle (-6, 2);
    \node[scale=1.1] at (-5, 1.5) {$\matr{A} = \begin{bmatrix}3 & 2 \\ 0 & 2\end{bmatrix}$};
    \end{tikzpicture}
\end{center}
\vspace{-8pt}
%Singular (non-invertible) matrices ``squish'' the space into a smaller dimension and therefore have a determinant of zero. The identity matrix $\matr{I}$ leaves a space unchanged and as such has a determinant of $1$.


\paragraph{The Pivot Formula}

When $\matr{A}$ is an upper or lower triangular matrix, its determinant is equal to the product of its pivots along its diagonal:
\begin{equation*}
\det \matr{A} = a_{11} a_{22} \cdots a_{nn} = \prod_{i = j} a_{ij}.
\end{equation*}

In order to apply this formula to non-triangular matrices, perform elimination to get $\matr{A=LU}$. Then we have
\begin{equation*}
\det \matr{A} = (\det \matr{U})(\det \matr{L}) = (1)(\det \matr{U}) = \det \matr{U}
\end{equation*}

This is usually the most practical way to compute the determinant

\paragraph{The Cofactor Formula} \label{det-cofactor}

Let $\matr{M}_{ij}$ be the submatrix obtained by removing the $i$th row and the $j$th column from some matrix $\matr{A}$. The cofactor $C_{ij}$ of the entry $a_{ij}$ of $\matr{A}$ is the number $(-1)^{i+j} \det \matr{M}_{ij}$. The determinant $\det \matr{A}$ may then be given by the dot product of any row (or column) of $\matr{A}$ with its cofactors, e.g.:

\begin{equation*}
\boxed{\det \matr{A} = a_{11} \matr{C}_{11} + a_{12} \matr{C}_{12} + \dots + a_{1n} \matr{C}_{1n}}
\end{equation*}

The cofactor formula can be visualized as follows.
\begin{gather*}
%\begin{vmatrix}
%a_{11} & a_{12} & a_{13} \\
%a_{21} & a_{22} & a_{23} \\
%a_{31} & a_{32} & a_{33}
%\end{vmatrix}
%= \\ 
a_{11} \begin{vmatrix}
\blacksquare & \square & \square \\
\square & a_{22} & a_{23} \\
\square & a_{32} & a_{33}
\end{vmatrix}
- a_{12} \begin{vmatrix}
\square & \blacksquare & \square \\
a_{21} & \square & a_{23} \\
a_{31} & \square & a_{33}
\end{vmatrix}
+ a_{13} \begin{vmatrix}
\square & \square & \blacksquare \\
a_{21} & a_{22} & \square  \\
a_{31} & a_{32} & \square 
\end{vmatrix}
\end{gather*}
Note that this pattern extends to matrices of arbitrary dimensions.

\subsection{Inverse of a Matrix}
\begin{gather*}
\matr{A}\matr{A}^{-1} = \matr{A}^{-1}\matr{A} = I, \quad \matr{A} \in \vecspace{R}^{n\times n} \quad (\text{if } \mathrm{rank} \, \matr{A} = n)\\
\begin{bmatrix}
a & b \\ c & d
\end{bmatrix}^{-1}
= \frac{1}{ad-bc}\begin{bmatrix}
d & -b \\ -c & a
\end{bmatrix},\quad \left[\,\matr{A} \; \vert \; \matr{I}\,\right] 
\sim \left[\,\matr{I} \; \vert \; \matr{A}^{-1}\,\right]
\end{gather*}



\section{Vector Spaces}
A vector space $\vecspace{V}$ is a set that is closed under vector addition and scalar multiplication. The scalars are members of a field $\vecspace{F}$.
In order for $\vecspace{V}$ to be a vector space over $\vecspace{F}$, the following conditions must hold true for all vectors $\vec u, \vec v, \vec w \in \vecspace{V}$ and scalars $c,d \in \vecspace{F}$.
\begin{multicols}{2}
\begin{enumerate}[leftmargin=*]
    % See strang 131
%    \item Vector addition commutes
    \item $\vec u+ \vec v=\vec v+\vec u$

    
%    \item Vector addition is associative
    \item $(\vec u+\vec v)+\vec w=\vec u+(\vec v+\vec w)$
    
%    \item Additive identity exists for vector addition
    \item $\exists \vec{0} \quad \vec{0}+\vec{v}=\vec{v}$
%    \begin{equation*}\exists! \, \bvec{0} \in \vecspace{V} \hspace{0.5cm}
%    \forall \bvec{v} \in \vecspace{V} \quad \bvec{0+v=v}\end{equation*}
    
%    \item Additive inverse exists for vector addition
    \item $\exists (-\vec{v}) \quad v+(-v)=0$
%    \begin{equation*}\forall \bvec{v} \in \vecspace{V} \hspace{0.5cm}
%    \exists! \, \bvec{(-v)} \in \vecspace{V} \quad \bvec{v+(-v)=0}\end{equation*}

%    \item Multiplicative identify exists for scalar multiplication
    \item $\exists \vec{1} \quad \vec{1}\vec{v}=\vec{v}$
%    \begin{equation*}\exists! \, 1 \in \vecspace{F} \hspace{0.5cm}
%    \forall \bvec{v} \in \vecspace{V} \quad 1\bvec{v=v}\end{equation*}
    
%    \item Scalar multiplication is associative
    \item $(cd)\vec{v}=c(d\vec{v})$
%    \begin{equation*}\forall c,d \in \vecspace{F} \quad
%    \forall \bvec{v} \in \vecspace{V} \quad (cd)\bvec{v}=c(d\bvec{v})\end{equation*}
    
%    \item Scalar multiplication distributes over vector addition
    \item $c(\vec{u}+\vec{v}) = c\vec{u} + c\vec{v}$
%    \begin{equation*}\forall c \in \vecspace{F} \quad
%    \forall \bvec{u,v} \in \vecspace{V} \quad c(\bvec{u+v}) = c\bvec{u} + d\bvec{v}\end{equation*}
    
%    \item Scalar multiplication distributes over scalar addition
    \item $(c+d)\vec{v} = c\vec{v} + d\vec{v}$
%    \begin{equation*}\forall c,d \in \vecspace{F} \quad
%    \forall \bvec{v} \in \vecspace{V} \quad (c+d)\bvec{v} = c\bvec{v} + d\bvec{v}\end{equation*}
    
\end{enumerate}
\end{multicols}

\paragraph{Subspace}
A subspace of a vector space is a set of vectors whose linear combinations stay within the subspace. That is, it is a set of vectors that is closed under vector addition and scalar multiplication. Note that all subspaces include the zero vector $\vec{0}$.

\paragraph{Linear Combination}
Given a sequence of vectors $\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_n$, a linear combination of the vectors is a vector $c_1\vec{v}_1 + c_2\vec{v}_2 + \cdots + c_n\vec{v}_n$ where $c_1, c_2, \ldots, c_n$ are scalar.

\paragraph{Span} The span of a set of vectors $\left\{\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_n\right\}$ in a vectorspace $\vecspace{V}$ is a subspace of $\vecspace{V}$ that contains all of the vectors' linear combinations. It is often denoted $\mathrm{span}\, \{\vec{v}_i\}$.
\begin{equation*}
\mathrm{span} \, \{\vec{v}_1, \dots, \vec{v}_m\} = \left \{ { \left. \sum_{i=1}^m c_i \vec{v}_i \, \right| c_i \in \vecspace{R} } \right \}
\end{equation*}
Note that the span is itself a vector space.

\paragraph{Basis}
A sequence of vectors $\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_n$ is a basis for a vector space $\vecspace{V}$ if
\begin{enumerate}[noitemsep]
    \item $\vecspace{V}= \mathrm{span}\left\{\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_n\right\}$
    \item The vectors $\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_n$ are linearly independent.
\end{enumerate}

\paragraph{Dimension} The number of vectors in a basis for a vector space $\vecspace{V}$ is called the dimensions of $\vecspace{V}$, denoted $\dim \vecspace{V}$.

\paragraph{Nullspace}
The nullspace $\mathrm{N}(\matr{A})$ of a matrix $\matr{A}$ is the set of vectors $\vec{v}$ satisfying $\matr{A}\vec{v} = \vec{0}$.

\section{Eigenvectors and Eigenvalues}
Given a matrix $\matr{A}\in\vecspace{R}^{n\times n}$, an \textbf{eigenvector} of $\matr{A}$ in a vector $\vec{x}\in\vecspace{R}^n$ such that $\matr{A}\vec{x} = \lambda\vec{x}$ for some scalar $\lambda$. The scalar $\lambda$ is said to be an \textbf{eigenvalue} of $\matr{A}$.
\begin{description}[style=unboxed,leftmargin=\cheatsheetmargin+0.2cm, topsep=0.2cm]
    \item[Computing Eigenvalues]
        Solve for each scalar $\lambda$ such that
        \begin{gather*}
            \matr{A}\vec{x} = \lambda\vec{x} \implies \matr{A}\vec{x} - \lambda \matr{I}\vec{x} = \vec{0} \implies \boxed{\det \left( \matr{A}\vec{x} - \lambda \matr{I}\right)\vec{x} = 0}
        \end{gather*}
        
        For each eigenvalue $\lambda_i$, find the associated eigenvector by solving for the nullspace of the matrix $\matr{A}\vec{x} - \lambda \matr{I}$.
        \begin{equation*}
            \vec{x}_i \in \mathrm{N}(\matr{A}\vec{x} - \lambda_i \matr{I})
        \end{equation*}
    
    \item[Algebraic Multiplicity (AM)] The number of times the eigenvalue $\lambda$ is repeated.
    \item[Geometric Mulitplicity (GM)] The number of linearly independent eigenvectors associated with the eigenvalue $\lambda_i$. (Equals dimension of the nullspace of $\matr{A}\vec{x} - \lambda_i \matr{I}$)
        
    \item[Properties of Eigenvalues]\hspace{\textwidth}\\
        \begin{itemize}[leftmargin=*]
            \item The sum of the eigenvalues equals the trace of a matrix.
            \item The product of the eigenvalues equals the determinant of a matrix
        \end{itemize}
    
    \item[Complex Eignenvalues]\hspace{\textwidth}\\
        \begin{enumerate}[leftmargin=*]
             \item If $\lambda_1 = a+ib$ is an eigenvalue of a matrix $\matr{A}$, then $\lambda_2 = a-ib$ is also an eigenvalue of $\matr{A}$.
            \item If $\vec{v}_1 = \vec{x}+i\vec{y}$ is an eigenvalue for $\lambda_1$, then $\vec{v}_2 = \vec{x}-i\vec{y}$ is an eigenvector for $\lambda_2$.
        \end{enumerate}
\end{description}
\section{Systems of Differentials Equations}

A system of differential equations in $n$ variables $x_1,\ldots,x_n$ is of the form
\begin{equation*}
\vec{x}' = \begin{bmatrix}
x'_1(t) \\ \vdots \\ x'_n (t)
\end{bmatrix}
=\begin{bmatrix}
a_{11} & \cdots & a_{1n} \\
\vdots & \ddots & \vdots \\
 a_{n1} & \cdots & a_{nn} \\
\end{bmatrix} \begin{bmatrix}
x_1(t) \\ \vdots \\ x_n (t)
\end{bmatrix} + \begin{bmatrix}
b_1 \\ \vdots \\ b_n
\end{bmatrix} = \matr{A}\vec{x} + \vec{b}
\end{equation*}
If $\vec{b} = \vec{0}$, the system is homogeneous.

\subsection{General Homogeneous Solution}
Assume $\vec{x} = e^{\lambda t}\vec{v}$ is a solution. Then,
\begin{equation*}
    \vec{x}' = \lambda e^{\lambda t}\vec{v} \implies \lambda e^{\lambda t}\vec{v} = \matr{A}e^{\lambda t}\vec{v} \implies (\matr{A}-\lambda\matr{I})\vec{v} = \vec{0}
\end{equation*}
\begin{equation*}
    \boxed{\vec{x}' = c_1 e^{\lambda_1 t} \vec{v}_1 + c_2 e^{\lambda_2 t} \vec{v}_2 + \cdots + c_n e^{\lambda_n t} \vec{v}_n}
\end{equation*}

\subsection{Systems with Complex Eigenvalues}
Assume $\matr{A}$ has two eigenvalues $\lambda_1 = a + ib$ and $\lambda_2 = a-ib$ with eigenvectors $\vec{v}_1$ and $\vec{v}_2$, respectively.
\begin{enumerate}
    \item Start with any eigenvector-eigenvalue pair, say $e^{\lambda_1t}\vec{v}_1$
    \item Simplify and separate real and imaginary parts
    \begin{align*}
        e^{(a+ib)t}\vec{v}_1 &= e^{at} \left[\cos (bt) + i\sin (bt)  \right]\begin{bmatrix}
        v_1 \\ v_2
        \end{bmatrix}\\
        &= e^{at}\begin{bmatrix}
        (\cos (bt) + i\sin (bt))v_1 \\ (\cos (bt) + i\sin (bt))v_2
        \end{bmatrix}
        =e^{at} \left[\vec{w}_1 + i \vec{w}_2 \right] 
    \end{align*}
    \item The homogeneous solution is then given by
    \begin{equation*}
        \vec{x}(t) = c_1 e^{at}\vec{w}_1 + c_2 e^{at} \vec{w}_2
    \end{equation*}
\end{enumerate}

%\begin{bmatrix}
%    \Re [v_1] + i \Im[v_1] \\ \Re [v_2] + i \Im[v_2]
%\end{bmatrix}

\end{multicols}

\end{document}